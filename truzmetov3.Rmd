---
title: 'CS7641 Assignment II: Prediction via Randomized Optimization'
author: "T. Ruzmetov"
date: "October 10, 2017"
output:
    pdf_document:
        fig_caption: yes
    
---

\fontfamily{cmr}
\fontsize{11}{22}
\selectfont

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

## Introduction
Purpose of this work is to understand how various randomized optimization techniques like "Hill Climbing",
"Simulated Annealing", "Genetic Algorithm" and "MIMIC" performe on different problem sets such as "Continuous Peaks Problem",
"Traveling Salesman" and "Knapsack" problem. In addition, we are asked to test performance of ANN with randomized
optimization methods such as "Hill Climbing", "Simulated Annealing" and "Genetic Algorithm" against ANN with backpropogation
method we used on one of the datasets from assignment one.  


### Data Preparation

Adult data set is used making exactly same train test splitting as in a previous assignment. All multi class categorical
variables are mapped into binary class numeric variables[0,1], which increased number of features from 11 to 66. Data preprocessing
is performed with R language and stored as csv inside clean_data folder. 


### Randomized Hill Climbing (RHC)

RHC method is designned to find optimal(usually maximum of fitness function) by randomly choosing starting position
and moving untill turning point is reached. By performing many iterations RHC chooses best optima among all local
optimas found. RHC method is fast, but it is not robust in its search for global optima in situations where there are 
many local peaks. I performed RHC using RandomizedHillClimbing in ABAGAIL via Jython.

### Simulated Annealing (SA)

Simulated Annealing is very widespread optimization technique especially in physics, where it is mostly
used for energy minimization problems. To find global optima, it randomly samples different states by accepting them
based on detailed balance condition, while slowly cooling the system. The algorithm, a function of initial temperature
and cooling rate, strikes a balance between exploring new points and exploiting nearby neighbors in search of local optima.
I used SimulatedAnnealing in ABAGAIL via Jython.

### Genetic Algorithm (GA)

Inspired by biology, in Genetic Algorithm the population evolves by iteratively mating and mutating parts to
crossover the best traits and to eliminate irrelevant traits. Large hypothesis space is a major disadvantage of GA,
which is dictated exponentially by the number of attributes. SA was performed using StandardGeneticAlgorithm in ABAGAIL on
Java+Jython.


### Mutual-Information-Maximizing Input Clustering (MIMIC)

In contrast to most optimization algorithms MIMIC algorithm is unique in its ability to  “remember” previous
iterations and use sampled probability densities to build structure of the solution space in order to locate global 
optima. MIMIC was performed using MIMIC built into ABAGAIL with Java via Jython.


```{r, echo=FALSE}
library("lattice")
library("plyr")
library("ggplot2")
library("Rmisc")
```


```{r, echo=FALSE}
# input the data
col_names <- c("algo_type", "Fitness", "Niters", "Time")
co_peaks <- read.table("output/count_peaks.txt", sep = "", header = FALSE, col.names = col_names)
knapsack <- read.table("output/knapsack.txt", sep = "", header = FALSE, col.names = col_names)
tr_salesman <- read.table("output/tr_sales.txt", sep = "", header = FALSE, col.names = col_names)

# extract routes from salesman data
rout_data <- tr_salesman[tr_salesman$algo_type == "Rout_RHC"  |
                           tr_salesman$algo_type == "Rout_SA" |
                           tr_salesman$algo_type == "Rout_GA" | 
                           tr_salesman$algo_type == "Rout_MIMIC",]
names(rout_data) <- c("algo_type", "rout_sequence", "X", "Y" )

tr_salesman <- tr_salesman[tr_salesman$algo_type == "RHC" |
                           tr_salesman$algo_type == "SA"  |
                           tr_salesman$algo_type == "GA"  |
                           tr_salesman$algo_type == "MIMIC",]
```


## Continuous Peaks

In Continuous Peaks problem we are given a fitness function in 1D space with multiple optimas, where the main task
is to find global optima. Given its simplicity it is very good candidate for learning purpose. I tested the performance of above mentioned 4 randomized optimization methods on this problem by making some modifications into provided Jython code
under ABAGAIL package. 

```{r,echo=FALSE,fig.height=3.5,fig.width=7,fig.cap= "Continuous Peaks Performance"}

p1 <- ggplot(co_peaks , aes(x= Niters, y=Fitness, colour = algo_type)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = "", x = "Niters", y = "Fitness", color="") +
  theme(legend.position = "none",
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

p2 <- ggplot(co_peaks , aes(x= Niters, y=Time, colour = algo_type)) +
    geom_line() + 
    geom_point() +
    theme_bw() +
    labs(title = "", x = "Niters", y = "Clock Time", color="") +
    theme(legend.position = c(0.8,0.7),
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

multiplot(p1,p2,cols=2)
```

I made comparison between models by plotting fitness function versus number of randomized iterations as depicted in Fig.1. As shown,
GA and MIMIC models gave pest performance by achieving optimium value at about Fitness=112. One has to note that due to its extreme time
consumption MIMIC model was iterated only 1000 times and yet achived best performance. For a large number of iterations Genetic Algorithm 
resulted in best performence. It reached near optimum value after 10000 iterations which is a late when compared to other methods. The worst 
performance is shown by RHC, where it could only reach Fitness=80 which is probably local optima near global one. Overall, SA and RHC algorithms
are the fastest in performance. MIMIC was the slowest and GA was somewhere in the middle. 


\pagebreak 

## Traveling Salesman

In travelling salesman problem one has to choose a rout with minimum net distance between multiple cities assuming each city is visited once.
We make a little adjustment to a problem by choosing a fitness function as inverse of net distance traveled, so that searching for max of fitness
function is equivalent to searching for min distance.


```{r,echo=FALSE,fig.height=3.5,fig.width=7,fig.cap= "Traveling Salesman Performance"}
p1 <- ggplot(tr_salesman , aes(x= Niters, y= Fitness, colour = algo_type)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = " ", x = "Niters", y = "Optimum Distance", color="") +
  theme(legend.position = c(0.8,0.5),
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

p2 <- ggplot(tr_salesman , aes(x= Niters, y=Time, colour = algo_type)) +
    geom_line() + 
    geom_point() +
    theme_bw() +
    labs(title = "", x = "Niters", y = "Clock Time", color="") +
    theme(legend.position = "none",
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

multiplot(p1,p2,cols=2)
```




```{r,echo=FALSE,fig.height=3.0,fig.width=10,fig.cap= "Final best model path for all four algorithms"}
RHC <- rout_data[rout_data$algo_type == "Rout_RHC",]
SA <- rout_data[rout_data$algo_type == "Rout_SA",]
GA <- rout_data[rout_data$algo_type == "Rout_GA",]
MIMIC <- rout_data[rout_data$algo_type == "Rout_MIMIC",]

par(mfrow=c(1,4))
plot(RHC$X,RHC$Y, type="o", xlab="X", ylab="Y", col = "blue", lwd = 2, main = "RHC")
plot(SA$X,SA$Y, type="o", xlab="X", ylab="", yaxt='n', col = "red", lwd = 2, main = "SA")
plot(GA$X,GA$Y, type="o", xlab="X", ylab="", yaxt='n', col = "cyan", lwd = 2, main = "GA")
plot(MIMIC$X,MIMIC$Y, type="o", xlab="X", ylab="", yaxt='n', col = "orange", lwd = 2, main = "MIMIC")

```


## Knapcack

```{r,echo=FALSE,fig.height=3.5,fig.width=7,fig.cap= "Knapsack Problem Performance"}

p1 <- ggplot(knapsack , aes(x= Niters, y= Fitness, colour = algo_type)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = " ", x = "Niters", y = "Fitness", color="") +
  theme(legend.position = c(0.8,0.3),
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

p2 <- ggplot(knapsack , aes(x= Niters, y=Time, colour = algo_type)) +
    geom_line() + 
    geom_point() +
    theme_bw() +
    labs(title = "", x = "Niters", y = "Clock Time", color="") +
    theme(legend.position = "none",
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

multiplot(p1,p2,cols=2)
```


## ANN with Randomized Optimization 


```{r, echo=FALSE,fig.height=3,fig.width=5,fig.cap= "Randomized Hill Climbing on Adult Data"}
col_names <- c("iters","RMSE_train","RMSE_test")
AD_rhc <- read.table("output/AD_results_rhc", sep = "", header = FALSE, col.names = col_names)

p <- ggplot(AD_rhc , aes(x=iters)) +
  geom_line(aes(y = sqrt(RMSE_train), colour = "train")) + 
  geom_line(aes(y = sqrt(RMSE_test)+0.01, colour = "test")) +
  #geom_point() +
  theme_bw() +
  labs(title = "", x = "Niters", y = "RMSE", color="") +
  theme(legend.position =c(0.7,0.8), axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))
p

```



```{r, echo=FALSE,fig.height=3.5,fig.width=8,fig.cap= "Simulated Annealing Cooling"}
col_names <- c("iters","RMSE_train","RMSE_test","Temp")
AD_sa_cv <- read.table("output/AD_resultsCV_temp", sep = "", header = FALSE, col.names = col_names)
AD_sa_cv$Temp <- as.factor(AD_sa_cv$Temp)

p1 <- ggplot(AD_sa_cv , aes(x= iters, y= sqrt(RMSE_train), colour = Temp)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = "training error", x = "Niters", y = "RMSE", color="") +
  theme(legend.position ="none", axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

p2 <- ggplot(AD_sa_cv , aes(x= iters, y= sqrt(RMSE_test), colour = Temp)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = "testing error", x = "Niters", y = "", color="Temperature") +
  theme(legend.position =c(0.8,0.7),
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

multiplot(p1,p2,cols=2)
```


```{r, echo=FALSE,fig.height=3.5,fig.width=8,fig.cap= "Genetic Algorithm RMSE for different parameters"}
col_names <- c("iters","RMSE_train","RMSE_test","tuning")
AD_ga_cv <- read.table("output/AD_resultsCV_ga", sep = "", header = FALSE, col.names = col_names)
AD_ga_cv$tuning <- as.factor(AD_ga_cv$tuning)

p1 <- ggplot(AD_ga_cv , aes(x= iters, y= sqrt(RMSE_train), colour = tuning)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = "training error", x = "Niters", y = "RMSE", color="") +
  theme(legend.position ="none", axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

p2 <- ggplot(AD_ga_cv , aes(x= iters, y= sqrt(RMSE_test), colour = tuning)) +
  geom_line() + 
  geom_point() +
  theme_bw() +
  labs(title = "testing error", x = "Niters", y = "", color="tuning") +
  theme(legend.position =c(0.8,0.7),
        axis.title = element_text(size = 12.0),
        axis.text = element_text(size=8, face = "bold"),
        plot.title = element_text(size = 12, hjust = 0.5),
        #text = element_text(family="Times New Roman"),
        axis.text.x = element_text(colour="black"),
        axis.text.y = element_text(colour="black"))

multiplot(p1,p2,cols=2)
```


